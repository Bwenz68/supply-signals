import os
import json
import argparse
from pathlib import Path
from datetime import datetime, timezone
from typing import Any, Dict

from .cik_ticker_map import load_map

IN_DIR  = Path(os.getenv("RAW_QUEUE_DIR", "queue/raw_events"))
OUT_DIR = Path(os.getenv("NORM_QUEUE_DIR", "queue/normalized_events"))

def to_iso_utc(ts: str):
    """
    Coerce common timestamp strings -> ISO-8601 UTC.
    Returns None if parsing fails.
    """
    if not ts:
        return None
    try:
        # ISO-like: 2025-10-04T12:00:00Z
        return datetime.fromisoformat(ts.replace("Z", "+00:00")).astimezone(timezone.utc).isoformat()
    except Exception:
        pass
    # RFC-822-ish: Sat, 04 Oct 2025 12:30:00 GMT / +0000
    for fmt in ("%a, %d %b %Y %H:%M:%S %Z", "%a, %d %b %Y %H:%M:%S %z"):
        try:
            return datetime.strptime(ts, fmt).astimezone(timezone.utc).isoformat()
        except Exception:
            pass
    return None

def normalize_one(d: Dict[str, Any], refmap):
    # Detect format: new (pr_feed_cli/sec_edgar_cli) vs old (ingestors)
    src = d.get("source")
    meta = d.get("meta") or {}
    
    # New format detection
    # New format detection
    is_new_format = src in ("pr_feed", "sec_edgar", "press_release")
    
    if is_new_format:
        # New format from pr_feed_cli.py / sec_edgar_cli.py
        issuer_name = d.get("issuer_name")
        cik = d.get("cik")
        ticker = None
        event_kind = d.get("event_kind") or src
        event_subtype = d.get("form_type")
        ts = d.get("event_datetime")
        urls = [d.get("first_url")] if d.get("first_url") else []
        title = d.get("title")
        body = d.get("summary")
        
        # Normalize CIK if present
        if cik:
            cik = cik.lstrip("0") or None
            if cik and cik in refmap:
                ticker = ticker or refmap[cik]["ticker"]
                issuer_name = issuer_name or refmap[cik]["company"]
    else:
        # Old format from press_release_ingestor.py / sec_edgar_ingestor.py
        cik = (meta.get("cik") or "").lstrip("0") or None
        ticker = meta.get("ticker")
        issuer_name = meta.get("company_name")
        
        if cik and (cik in refmap):
            ticker = ticker or refmap[cik]["ticker"]
            issuer_name = issuer_name or refmap[cik]["company"]
        
        event_kind = "SEC" if src == "SEC" else ("PR" if src == "PR" else "OTHER")
        event_subtype = meta.get("doc_type")
        ts = d.get("ts")
        urls = meta.get("urls") or []
        title = d.get("title")
        body = d.get("body")
    
    # Build normalized record - ADDITIVE (keep original fields + add enrichments)
    norm = dict(d)  # Start with all original fields
    
    # Phase-0 baseline (ensure present)
    norm["title"] = title
    norm["body"] = body
    norm["source"] = src
    if ts:
        norm["ts"] = ts
    
    # Phase-1 enrichments (additive)
    if issuer_name:
        norm["issuer_name"] = issuer_name
        norm["canonical_company"] = issuer_name
    if ticker:
        norm["ticker"] = ticker
        norm["canonical_ticker"] = ticker
    if cik:
        norm["cik"] = cik
        norm["canonical_cik"] = cik
    
    # Timestamp hardening
    if ts:
        dt = to_iso_utc(ts)
        if dt:
            norm["event_datetime_utc"] = dt
    
    norm["event_kind"] = event_kind
    if event_subtype:
        norm["event_subtype"] = event_subtype
    
    # URL handling
    if urls:
        norm["urls"] = urls
        if not norm.get("first_url") and urls:
            norm["first_url"] = urls[0]
    
    return norm

def main():
    ap = argparse.ArgumentParser(description="Normalize raw events to Phase-0-compatible records with optional enrichments.")
    ap.add_argument("--once", action="store_true", help="Process all NDJSON in IN_DIR once and exit.")
    args = ap.parse_args()

    refmap = load_map()
    OUT_DIR.mkdir(parents=True, exist_ok=True)

    in_files = sorted(IN_DIR.glob("*.jsonl"))
    total_out = 0

    for fp in in_files:
        out_fp = OUT_DIR / fp.name.replace(".jsonl", ".norm.jsonl")
        count = 0
        with fp.open("r", encoding="utf-8") as f, out_fp.open("w", encoding="utf-8") as g:
            for line in f:
                raw = json.loads(line)
                norm = normalize_one(raw, refmap)
                g.write(json.dumps(norm, ensure_ascii=False) + "\n")
                count += 1
                total_out += 1
        print(f"[NORMALIZE] {fp.name} -> {out_fp.name} ({count} items)")

    print(f"[NORMALIZE] wrote {total_out} normalized items")

if __name__ == "__main__":
    main()
